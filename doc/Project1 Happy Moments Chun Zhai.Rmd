---
title: "Project1 Happy Moments Chun Zhai"
author: "Chun Zhai"
date: "September 10, 2018"
output: html_document
runtime: shiny
---

```{r load libraries, warning=FALSE, message=FALSE}

library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ngram)
library(shiny) 
library(ggplot2)
library(reshape2)
library(lemon)
```

### Step 1 - Load the processed text data along with demographic information on contributors

We use the processed data for our analysis and combine it with the demographic information available.

```{r load data, warning=FALSE, message=FALSE}
hm_data <- read_csv("../output/processed_moments.csv")

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
```

### Combine both the data sets and keep the required columns for analysis

```{r combining data, warning=FALSE, message=FALSE}
hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))
```

```{r}
datatable(hm_data)
```

### Create a bag of words using the text data
 
```{r bag of words, warning=FALSE, message=FALSE}
bag_of_words <-  hm_data %>%
  unnest_tokens(word, text)

word_count <- bag_of_words %>%
  count(word, sort = TRUE)
```


```{r shiny UI, warning=FALSE, message=FALSE}
ui <- navbarPage("What makes people happy?",
                 tabPanel("Overview",
                          
                          titlePanel(h1("Most Frequent Occurrences",
                                        align = "center")),
                          
                          sidebarLayout(
                            sidebarPanel(
                              sliderInput(inputId = "topWordcloud",
                                          label = "Number of terms for word cloud:",
                                          min = 5,
                                          max = 100,
                                          value = 50)
                            ),
                            
                            mainPanel(
                              wordcloud2Output(outputId = "WC")
                            )
                          )
                 )
)
```

### Develop the server for the R Shiny app

```{r shiny server, warning=FALSE, message=FALSE}
server <- function(input, output, session) {
  
  output$WC <- renderWordcloud2({
    
    word_count %>%
      slice(1:input$topWordcloud) %>%
      wordcloud2(size = 0.6,
                 rotateRatio = 0)
    
  })
}
```

### Look into some variables

I discover that great amount of people are from USA and India, and there are 99 countries in total.
```{r}
mycountry<-as.data.frame(table(hm_data$country))
mycountryupdated<-mycountry[order(mycountry$Freq, decreasing = T),]
mycountrytop10<-mycountryupdated[1:10,]

colnames(mycountrytop10) <- c("Country", "Count")
Countrylist<-c("USA", "India", "Venezuela", "Canada", "UK", "Philippines", "Vietnam", "Brazil", 
                         "Mexico", "Australia")
cbind(Countrylist,mycountrytop10)
```

There are more male being asked than famale, and more single being asked than married. The median and mean of age are 30 and 31.83 if I omit NA value and values larger than or equal to 120.
```{r} 
table(hm_data$gender)
table(hm_data$marital)

agesub<-na.omit(as.numeric(hm_data$age[which(as.numeric(hm_data$age)<120)]))
hist(agesub, main = 'Histogram of Age', xlab='Age',col="pink")
median(agesub)
mean(agesub)
```

### Explore text answers
```{r}
mytext<-as.data.frame(hm_data)[hm_data$country %in% mycountrytop10$Country,]
textcount<-nchar(mytext$original_hm)
textcountry<-mytext$country
textdataframe<-data.frame(textcountry,textcount)

ggplot(data = textdataframe, aes(x = textcount)) +
geom_histogram(color="darkblue", fill="lightblue") +
facet_wrap(~ textcountry, scales="free",nrow = 4)
```

Based on the shapes of above histograms, I find that different countries have different average length of answers, so I do more work on this.

```{r}
mean_of_text<-setNames(aggregate(textdataframe[, 2], list(textdataframe$textcountry), mean),c("Countries","Mean"))
mean_of_text
ggplot(mean_of_text, aes(x=Countries, y=Mean, color=Countries)) +
geom_point()

quantile_of_text<-aggregate(textdataframe[, 2], list(textdataframe$textcountry), quantile)
colnames(quantile_of_text)<-c("Countries","length")
quantile_of_text$length<-round(quantile_of_text$length)
quantile_of_text

s<-
"Countries length.0% length.25% length.50% length.75% length.100%
     AUS        13         41         56         93         297
     BRA        19         38         50         73         199
     CAN        16         42         60         86         497
     GBR        15         42         70        101        1233
     IND        11         41         79        151        6550
     MEX        15         43         60         90         267
     PHL        16         44         62         92         427
     USA         6         46         67         98        6557
     VEN        11         49         74        111         766
     VNM        15         31         46         63         336
"

d1 <- read.delim(textConnection(s), sep="")
d1 <- melt(d1, id.vars="Countries")

ggplot(d1, aes(Countries,value, col=variable)) + 
  geom_point() + 
  stat_smooth()
```

As we can see from the above graph, USA and India have some extreme values of length. But since the data range is too large to make further analysis, I decide to look at data for each quantile seperately.

```{r}
d2<- read.table(text = s, header = T)
d2 %>% tidyr::gather("id", "value", 2:6) %>% 
  ggplot(., aes(Countries, value))+
  geom_point( color="purple")+
  geom_smooth(method = "lm", se=FALSE)+
  facet_wrap(~id,scales="free", nrow=3)
```

India, Venezuela, USA and UK seem to have the highest mean and median caompare to other top 10 countries. So I decide to study more on these four countries.

```{r}
four_countries<-as.data.frame(hm_data)[hm_data$country %in% c('USA','IND','VEN','GBR'),]

split<-split(four_countries, four_countries$country)
list_fourcountries<-lapply(split, function(y) 
 output<-list(max(nchar(y$original_hm)),
              max(nchar(y$original_hm)[nchar(y$original_hm)!=max(nchar(y$original_hm))]),
              max(nchar(y$original_hm)[nchar(y$original_hm)!=max(nchar(y$original_hm))]
                  [nchar(y$original_hm)[nchar(y$original_hm)!=max(nchar(y$original_hm))]
                    !=max(nchar(y$original_hm)[nchar(y$original_hm)!=max(nchar(y$original_hm))])])
              )
)

list_fourcountries<-as.data.frame(list_fourcountries)
colnames(list_fourcountries)<-c('GBR1','GBR2','GBR3','IND1','IND2','IND3',
                                'USA1','USA2','USA3','VEN1','VEN2','VEN3')
list_fourcountries<-data.frame(t(list_fourcountries))
list_fourcountries<-cbind(countries = rownames(list_fourcountries), list_fourcountries) 
rownames(list_fourcountries)<-c()
colnames(list_fourcountries)<-c('Country_rank','Value')
list_fourcountries

ggplot(data=list_fourcountries, aes(x = Country_rank, y= Value, fill = Country_rank)) +
geom_bar(stat="identity")+
scale_colour_gradient2()+
coord_flip()
```

I select 3 original_hms that have the most charactors in the response sentences for each of the four countries (India, Venezuela, USA and UK), and I would like to see what these sentences are. 

```{r}
GBR1<-four_countries[which(nchar(four_countries$original_hm)==1233 
                           & four_countries$country=='GBR'),]
GBR2<-four_countries[which(nchar(four_countries$original_hm)==410 
                           & four_countries$country=='GBR'),]
GBR3<-four_countries[which(nchar(four_countries$original_hm)==318 
                           & four_countries$country=='GBR'),]

IND1<-four_countries[which(nchar(four_countries$original_hm)==6550 
                           & four_countries$country=='IND'),]
IND2<-four_countries[which(nchar(four_countries$original_hm)==6532 
                           & four_countries$country=='IND'),]
IND3<-four_countries[which(nchar(four_countries$original_hm)==5348 
                           & four_countries$country=='IND'),]

USA1<-four_countries[which(nchar(four_countries$original_hm)==6557 
                           & four_countries$country=='USA'),]
USA2<-four_countries[which(nchar(four_countries$original_hm)==2882 
                           & four_countries$country=='USA'),]
USA3<-four_countries[which(nchar(four_countries$original_hm)==2274 
                           & four_countries$country=='USA'),]

VEN1<-four_countries[which(nchar(four_countries$original_hm)==766 
                           & four_countries$country=='VEN'),]
VEN2<-four_countries[which(nchar(four_countries$original_hm)==727 
                           & four_countries$country=='VEN'),]
VEN3<-four_countries[which(nchar(four_countries$original_hm)==606 
                           & four_countries$country=='VEN'),]

sentence_fourcountries<-rbind(GBR1,GBR2,GBR3,IND1,IND2,IND3,USA1,USA2,USA3,VEN1,VEN2,VEN3)
sentence_fourcountries
```

### Run the R Shiny app

```{r shiny app, warning=FALSE, message=FALSE}
shinyApp(ui, server)
```
